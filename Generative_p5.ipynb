{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnr0/generative-p5.js/blob/main/Generative_p5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjF-zkn5nScx"
      },
      "outputs": [],
      "source": [
        "!ngrok authtoken [putyourauthtoken]\n",
        "!pip install torch torchvision diffusers transformers\n",
        "!pip install flask_ngrok flask\n",
        "!pip install pyngrok==4.1.1\n",
        "!pip install flask_cors\n",
        "!pip install opencv-python matplotlib\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "!pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "!pip install controlnet-aux==0.0.3\n",
        "!pip install xformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionControlNetPipeline\n",
        "from diffusers.image_processor import VaeImageProcessor\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
        "from diffusers.models import AutoencoderKL, UNet2DConditionModel, ControlNetModel\n",
        "from diffusers.schedulers import KarrasDiffusionSchedulers\n",
        "from transformers import CLIPImageProcessor, CLIPTextModel, CLIPTokenizer\n",
        "from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker\n",
        "from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_controlnet import MultiControlNetModel\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import torch\n",
        "from torch import nn\n",
        "from diffusers.utils import (\n",
        "    deprecate,\n",
        "    replace_example_docstring,\n",
        "    randn_tensor,\n",
        ")\n",
        "from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n",
        "\n",
        "EXAMPLE_DOC_STRING = \"\"\"\n",
        "    Examples:\n",
        "        ```py\n",
        "        >>> # !pip install opencv-python transformers accelerate\n",
        "        >>> from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
        "        >>> from diffusers.utils import load_image\n",
        "        >>> import numpy as np\n",
        "        >>> import torch\n",
        "        >>> import cv2\n",
        "        >>> from PIL import Image\n",
        "        >>> # download an image\n",
        "        >>> image = load_image(\n",
        "        ...     \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",
        "        ... )\n",
        "        >>> image = np.array(image)\n",
        "        >>> # get canny image\n",
        "        >>> image = cv2.Canny(image, 100, 200)\n",
        "        >>> image = image[:, :, None]\n",
        "        >>> image = np.concatenate([image, image, image], axis=2)\n",
        "        >>> canny_image = Image.fromarray(image)\n",
        "        >>> # load control net and stable diffusion v1-5\n",
        "        >>> controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\n",
        "        >>> pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "        ...     \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16\n",
        "        ... )\n",
        "        >>> # speed up diffusion process with faster scheduler and memory optimization\n",
        "        >>> pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "        >>> # remove following line if xformers is not installed\n",
        "        >>> pipe.enable_xformers_memory_efficient_attention()\n",
        "        >>> pipe.enable_model_cpu_offload()\n",
        "        >>> # generate image\n",
        "        >>> generator = torch.manual_seed(0)\n",
        "        >>> image = pipe(\n",
        "        ...     \"futuristic-looking woman\", num_inference_steps=20, generator=generator, image=canny_image\n",
        "        ... ).images[0]\n",
        "        ```\n",
        "\"\"\"\n",
        "\n",
        "class StableDiffusionControlNetImg2ImgPipeline(StableDiffusionControlNetPipeline):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vae: AutoencoderKL,\n",
        "        text_encoder: CLIPTextModel,\n",
        "        tokenizer: CLIPTokenizer,\n",
        "        unet: UNet2DConditionModel,\n",
        "        controlnet: Union[ControlNetModel, List[ControlNetModel], Tuple[ControlNetModel], MultiControlNetModel],\n",
        "        scheduler: KarrasDiffusionSchedulers,\n",
        "        safety_checker: StableDiffusionSafetyChecker,\n",
        "        feature_extractor: CLIPImageProcessor,\n",
        "        requires_safety_checker: bool = True,\n",
        "    ):\n",
        "        super().__init__(vae, text_encoder, tokenizer, unet, controlnet, scheduler, safety_checker, feature_extractor, requires_safety_checker)\n",
        "        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)\n",
        "\n",
        "    def prepare_latents(self, image, timestep, batch_size, num_images_per_prompt, dtype, device, generator=None, noise_latents=None):\n",
        "        if not isinstance(image, (torch.Tensor, PIL.Image.Image, list)):\n",
        "            raise ValueError(\n",
        "                f\"`image` has to be of type `torch.Tensor`, `PIL.Image.Image` or list but is {type(image)}\"\n",
        "            )\n",
        "\n",
        "        image = image.to(device=device, dtype=dtype)\n",
        "\n",
        "        batch_size = batch_size * num_images_per_prompt\n",
        "        if isinstance(generator, list) and len(generator) != batch_size:\n",
        "            raise ValueError(\n",
        "                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n",
        "                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n",
        "            )\n",
        "\n",
        "        if isinstance(generator, list):\n",
        "            init_latents = [\n",
        "                self.vae.encode(image[i : i + 1]).latent_dist.sample(generator[i]) for i in range(batch_size)\n",
        "            ]\n",
        "            init_latents = torch.cat(init_latents, dim=0)\n",
        "        else:\n",
        "            init_latents = self.vae.encode(image).latent_dist.sample(generator)\n",
        "\n",
        "        init_latents = self.vae.config.scaling_factor * init_latents\n",
        "\n",
        "        if batch_size > init_latents.shape[0] and batch_size % init_latents.shape[0] == 0:\n",
        "            # expand init_latents for batch_size\n",
        "            deprecation_message = (\n",
        "                f\"You have passed {batch_size} text prompts (`prompt`), but only {init_latents.shape[0]} initial\"\n",
        "                \" images (`image`). Initial images are now duplicating to match the number of text prompts. Note\"\n",
        "                \" that this behavior is deprecated and will be removed in a version 1.0.0. Please make sure to update\"\n",
        "                \" your script to pass as many initial images as text prompts to suppress this warning.\"\n",
        "            )\n",
        "            deprecate(\"len(prompt) != len(image)\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
        "            additional_image_per_prompt = batch_size // init_latents.shape[0]\n",
        "            init_latents = torch.cat([init_latents] * additional_image_per_prompt, dim=0)\n",
        "        elif batch_size > init_latents.shape[0] and batch_size % init_latents.shape[0] != 0:\n",
        "            raise ValueError(\n",
        "                f\"Cannot duplicate `image` of batch size {init_latents.shape[0]} to {batch_size} text prompts.\"\n",
        "            )\n",
        "        else:\n",
        "            init_latents = torch.cat([init_latents], dim=0)\n",
        "\n",
        "        shape = init_latents.shape\n",
        "        if noise_latents==None:\n",
        "          noise = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n",
        "        else:\n",
        "          noise = noise_latents\n",
        "        # get latents\n",
        "        init_latents = self.scheduler.add_noise(init_latents, noise, timestep)\n",
        "        latents = init_latents\n",
        "\n",
        "        return latents\n",
        "\n",
        "    def get_timesteps(self, num_inference_steps, strength, device):\n",
        "        # get the original timestep using init_timestep\n",
        "        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n",
        "\n",
        "        t_start = max(num_inference_steps - init_timestep, 0)\n",
        "        timesteps = self.scheduler.timesteps[t_start:]\n",
        "\n",
        "        return timesteps, num_inference_steps - t_start\n",
        "\n",
        "    def check_inputs(self, prompt, controlnet_image, height, width, callback_steps, negative_prompt=None, prompt_embeds=None, negative_prompt_embeds=None, controlnet_conditioning_scale=1.0):\n",
        "        if height % 8 != 0 or width % 8 != 0:\n",
        "            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n",
        "\n",
        "        if (callback_steps is None) or (\n",
        "            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n",
        "                f\" {type(callback_steps)}.\"\n",
        "            )\n",
        "\n",
        "        if prompt is not None and prompt_embeds is not None:\n",
        "            raise ValueError(\n",
        "                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n",
        "                \" only forward one of the two.\"\n",
        "            )\n",
        "        elif prompt is None and prompt_embeds is None:\n",
        "            raise ValueError(\n",
        "                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n",
        "            )\n",
        "        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n",
        "            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
        "\n",
        "        if negative_prompt is not None and negative_prompt_embeds is not None:\n",
        "            raise ValueError(\n",
        "                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n",
        "                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n",
        "            )\n",
        "\n",
        "        if prompt_embeds is not None and negative_prompt_embeds is not None:\n",
        "            if prompt_embeds.shape != negative_prompt_embeds.shape:\n",
        "                raise ValueError(\n",
        "                    \"`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but\"\n",
        "                    f\" got: `prompt_embeds` {prompt_embeds.shape} != `negative_prompt_embeds`\"\n",
        "                    f\" {negative_prompt_embeds.shape}.\"\n",
        "                )\n",
        "\n",
        "        # `prompt` needs more sophisticated handling when there are multiple\n",
        "        # conditionings.\n",
        "        if isinstance(self.controlnet, MultiControlNetModel):\n",
        "            if isinstance(prompt, list):\n",
        "                logger.warning(\n",
        "                    f\"You have {len(self.controlnet.nets)} ControlNets and you have passed {len(prompt)}\"\n",
        "                    \" prompts. The conditionings will be fixed across the prompts.\"\n",
        "                )\n",
        "\n",
        "        # Check `controlnet_image`\n",
        "        if controlnet_image!=None:\n",
        "          if isinstance(self.controlnet, ControlNetModel):\n",
        "              self.check_image(controlnet_image, prompt, prompt_embeds)\n",
        "          elif isinstance(self.controlnet, MultiControlNetModel):\n",
        "              if not isinstance(controlnet_image, list):\n",
        "                  raise TypeError(\"For multiple controlnets: `image` must be type `list`\")\n",
        "\n",
        "              # When `image` is a nested list:\n",
        "              # (e.g. [[canny_image_1, pose_image_1], [canny_image_2, pose_image_2]])\n",
        "              elif any(isinstance(i, list) for i in controlnet_image):\n",
        "                  raise ValueError(\"A single batch of multiple conditionings are supported at the moment.\")\n",
        "              elif len(controlnet_image) != len(self.controlnet.nets):\n",
        "                  raise ValueError(\n",
        "                      \"For multiple controlnets: `image` must have the same length as the number of controlnets.\"\n",
        "                  )\n",
        "\n",
        "              for image_ in controlnet_image:\n",
        "                  self.check_image(image_, prompt, prompt_embeds)\n",
        "          else:\n",
        "              assert False\n",
        "\n",
        "          # Check `controlnet_conditioning_scale`\n",
        "          if isinstance(self.controlnet, ControlNetModel):\n",
        "              if not isinstance(controlnet_conditioning_scale, float):\n",
        "                  raise TypeError(\"For single controlnet: `controlnet_conditioning_scale` must be type `float`.\")\n",
        "          elif isinstance(self.controlnet, MultiControlNetModel):\n",
        "              if isinstance(controlnet_conditioning_scale, list):\n",
        "                  if any(isinstance(i, list) for i in controlnet_conditioning_scale):\n",
        "                      raise ValueError(\"A single batch of multiple conditionings are supported at the moment.\")\n",
        "              elif isinstance(controlnet_conditioning_scale, list) and len(controlnet_conditioning_scale) != len(\n",
        "                  self.controlnet.nets\n",
        "              ):\n",
        "                  raise ValueError(\n",
        "                      \"For multiple controlnets: When `controlnet_conditioning_scale` is specified as `list`, it must have\"\n",
        "                      \" the same length as the number of controlnets\"\n",
        "                  )\n",
        "          else:\n",
        "              assert False\n",
        "\n",
        "    @torch.no_grad()\n",
        "    @replace_example_docstring(EXAMPLE_DOC_STRING)\n",
        "    def __call__(\n",
        "        self,\n",
        "        prompt: Union[str, List[str]] = None,\n",
        "        controlnet_image: Union[torch.FloatTensor, PIL.Image.Image, List[torch.FloatTensor], List[PIL.Image.Image]] = None,\n",
        "        height: Optional[int] = 512,\n",
        "        width: Optional[int] = 512,\n",
        "        num_inference_steps: int = 50,\n",
        "        guidance_scale: float = 7.5,\n",
        "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
        "        num_images_per_prompt: Optional[int] = 1,\n",
        "        eta: float = 0.0,\n",
        "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
        "        latents: Optional[torch.FloatTensor] = None,\n",
        "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        output_type: Optional[str] = \"pil\",\n",
        "        return_dict: bool = True,\n",
        "        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
        "        callback_steps: int = 1,\n",
        "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        controlnet_conditioning_scale: Union[float, List[float]] = 1.0,\n",
        "        init_image: Union[torch.FloatTensor, PIL.Image.Image, List[torch.FloatTensor], List[PIL.Image.Image]] = None,\n",
        "        strength = None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        Function invoked when calling the pipeline for generation.\n",
        "        Args:\n",
        "            prompt (`str` or `List[str]`, *optional*):\n",
        "                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n",
        "                instead.\n",
        "            controlnet_image (`torch.FloatTensor`, `PIL.Image.Image`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`,\n",
        "                    `List[List[torch.FloatTensor]]`, or `List[List[PIL.Image.Image]]`):\n",
        "                The ControlNet input condition. ControlNet uses this input condition to generate guidance to Unet. If\n",
        "                the type is specified as `Torch.FloatTensor`, it is passed to ControlNet as is. `PIL.Image.Image` can\n",
        "                also be accepted as an image. The dimensions of the output image defaults to `image`'s dimensions. If\n",
        "                height and/or width are passed, `image` is resized according to them. If multiple ControlNets are\n",
        "                specified in init, images must be passed as a list such that each element of the list can be correctly\n",
        "                batched for input to a single controlnet.\n",
        "            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
        "                The height in pixels of the generated image.\n",
        "            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
        "                The width in pixels of the generated image.\n",
        "            num_inference_steps (`int`, *optional*, defaults to 50):\n",
        "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
        "                expense of slower inference.\n",
        "            guidance_scale (`float`, *optional*, defaults to 7.5):\n",
        "                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n",
        "                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n",
        "                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n",
        "                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n",
        "                usually at the expense of lower image quality.\n",
        "            negative_prompt (`str` or `List[str]`, *optional*):\n",
        "                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n",
        "                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n",
        "                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n",
        "            num_images_per_prompt (`int`, *optional*, defaults to 1):\n",
        "                The number of images to generate per prompt.\n",
        "            eta (`float`, *optional*, defaults to 0.0):\n",
        "                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n",
        "                [`schedulers.DDIMScheduler`], will be ignored for others.\n",
        "            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n",
        "                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n",
        "                to make generation deterministic.\n",
        "            latents (`torch.FloatTensor`, *optional*):\n",
        "                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n",
        "                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
        "                tensor will ge generated by sampling using the supplied random `generator`.\n",
        "            prompt_embeds (`torch.FloatTensor`, *optional*):\n",
        "                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n",
        "                provided, text embeddings will be generated from `prompt` input argument.\n",
        "            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
        "                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n",
        "                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n",
        "                argument.\n",
        "            output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
        "                The output format of the generate image. Choose between\n",
        "                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n",
        "            return_dict (`bool`, *optional*, defaults to `True`):\n",
        "                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n",
        "                plain tuple.\n",
        "            callback (`Callable`, *optional*):\n",
        "                A function that will be called every `callback_steps` steps during inference. The function will be\n",
        "                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n",
        "            callback_steps (`int`, *optional*, defaults to 1):\n",
        "                The frequency at which the `callback` function will be called. If not specified, the callback will be\n",
        "                called at every step.\n",
        "            cross_attention_kwargs (`dict`, *optional*):\n",
        "                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n",
        "                `self.processor` in\n",
        "                [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).\n",
        "            controlnet_conditioning_scale (`float` or `List[float]`, *optional*, defaults to 1.0):\n",
        "                The outputs of the controlnet are multiplied by `controlnet_conditioning_scale` before they are added\n",
        "                to the residual in the original unet. If multiple ControlNets are specified in init, you can set the\n",
        "                corresponding scale as a list.\n",
        "        Examples:\n",
        "        Returns:\n",
        "            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n",
        "            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n",
        "            When returning a tuple, the first element is a list with the generated images, and the second element is a\n",
        "            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n",
        "            (nsfw) content, according to the `safety_checker`.\n",
        "        \"\"\"\n",
        "        # 0. Default height and width to unet\n",
        "        if controlnet_image!=None:\n",
        "          height, width = self._default_height_width(height, width, controlnet_image)\n",
        "\n",
        "\n",
        "        # 1. Check inputs. Raise error if not correct\n",
        "        self.check_inputs(\n",
        "            prompt,\n",
        "            controlnet_image,\n",
        "            height,\n",
        "            width,\n",
        "            callback_steps,\n",
        "            negative_prompt,\n",
        "            prompt_embeds,\n",
        "            negative_prompt_embeds,\n",
        "            controlnet_conditioning_scale,\n",
        "        )\n",
        "\n",
        "        # 2. Define call parameters\n",
        "        if prompt is not None and isinstance(prompt, str):\n",
        "            batch_size = 1\n",
        "        elif prompt is not None and isinstance(prompt, list):\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            batch_size = prompt_embeds.shape[0]\n",
        "\n",
        "        device = self._execution_device\n",
        "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "        # corresponds to doing no classifier free guidance.\n",
        "        do_classifier_free_guidance = guidance_scale > 1.0\n",
        "\n",
        "        if controlnet_image!=None:\n",
        "          if isinstance(self.controlnet, MultiControlNetModel) and isinstance(controlnet_conditioning_scale, float):\n",
        "              controlnet_conditioning_scale = [controlnet_conditioning_scale] * len(self.controlnet.nets)\n",
        "\n",
        "        # 3. Encode input prompt\n",
        "        prompt_embeds = self._encode_prompt(\n",
        "            prompt,\n",
        "            device,\n",
        "            num_images_per_prompt,\n",
        "            do_classifier_free_guidance,\n",
        "            negative_prompt,\n",
        "            prompt_embeds=prompt_embeds,\n",
        "            negative_prompt_embeds=negative_prompt_embeds,\n",
        "        )\n",
        "\n",
        "        # 4. Prepare image\n",
        "        if controlnet_image!=None:\n",
        "          if isinstance(self.controlnet, ControlNetModel):\n",
        "              controlnet_image = self.prepare_image(\n",
        "                  image=controlnet_image,\n",
        "                  width=width,\n",
        "                  height=height,\n",
        "                  batch_size=batch_size * num_images_per_prompt,\n",
        "                  num_images_per_prompt=num_images_per_prompt,\n",
        "                  device=device,\n",
        "                  dtype=self.controlnet.dtype,\n",
        "                  do_classifier_free_guidance=do_classifier_free_guidance,\n",
        "              )\n",
        "          elif isinstance(self.controlnet, MultiControlNetModel):\n",
        "              images = []\n",
        "\n",
        "              for image_ in controlnet_image:\n",
        "                  image_ = self.prepare_image(\n",
        "                      image=image_,\n",
        "                      width=width,\n",
        "                      height=height,\n",
        "                      batch_size=batch_size * num_images_per_prompt,\n",
        "                      num_images_per_prompt=num_images_per_prompt,\n",
        "                      device=device,\n",
        "                      dtype=self.controlnet.dtype,\n",
        "                      do_classifier_free_guidance=do_classifier_free_guidance,\n",
        "                  )\n",
        "\n",
        "                  images.append(image_)\n",
        "\n",
        "              controlnet_image = images\n",
        "          else:\n",
        "              assert False\n",
        "\n",
        "        # prepare init latent image\n",
        "        if init_image!=None:\n",
        "          init_image = self.image_processor.preprocess(init_image)\n",
        "\n",
        "        # 5. Prepare timesteps\n",
        "        self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
        "        if strength!=None and init_image!=None:\n",
        "          timesteps, num_inference_steps = self.get_timesteps(num_inference_steps, strength, device)\n",
        "          latent_timestep = timesteps[:1].repeat(batch_size * num_images_per_prompt)\n",
        "        else:\n",
        "          timesteps = self.scheduler.timesteps\n",
        "\n",
        "        # 6. Prepare latent variables\n",
        "        # num_channels_latents = self.unet.in_channels\n",
        "        if strength==None or init_image==None:\n",
        "          latents = latents\n",
        "        else:\n",
        "          latents = self.prepare_latents(\n",
        "              init_image, latent_timestep, batch_size, num_images_per_prompt, prompt_embeds.dtype, device, generator, latents\n",
        "              # batch_size * num_images_per_prompt,\n",
        "              # num_channels_latents,\n",
        "              # height,\n",
        "              # width,\n",
        "              # prompt_embeds.dtype,\n",
        "              # device,\n",
        "              # generator,\n",
        "              # latents,\n",
        "          )\n",
        "\n",
        "        \n",
        "        # 7. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
        "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
        "\n",
        "        # 8. Denoising loop\n",
        "        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
        "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
        "            for i, t in enumerate(timesteps):\n",
        "                # expand the latents if we are doing classifier free guidance\n",
        "                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "                # controlnet(s) inference\n",
        "                if controlnet_image!=None:\n",
        "                  down_block_res_samples, mid_block_res_sample = self.controlnet(\n",
        "                      latent_model_input,\n",
        "                      t,\n",
        "                      encoder_hidden_states=prompt_embeds,\n",
        "                      controlnet_cond=controlnet_image,\n",
        "                      conditioning_scale=controlnet_conditioning_scale,\n",
        "                      return_dict=False,\n",
        "                  )\n",
        "\n",
        "                  # predict the noise residual\n",
        "                  noise_pred = self.unet(\n",
        "                      latent_model_input,\n",
        "                      t,\n",
        "                      encoder_hidden_states=prompt_embeds,\n",
        "                      cross_attention_kwargs=cross_attention_kwargs,\n",
        "                      down_block_additional_residuals=down_block_res_samples,\n",
        "                      mid_block_additional_residual=mid_block_res_sample,\n",
        "                  ).sample\n",
        "                else:\n",
        "                  # predict the noise residual\n",
        "                  noise_pred = self.unet(\n",
        "                      latent_model_input,\n",
        "                      t,\n",
        "                      encoder_hidden_states=prompt_embeds,\n",
        "                      cross_attention_kwargs=cross_attention_kwargs\n",
        "                  ).sample\n",
        "\n",
        "\n",
        "                # perform guidance\n",
        "                if do_classifier_free_guidance:\n",
        "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "                # compute the previous noisy sample x_t -> x_t-1\n",
        "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
        "\n",
        "                # call the callback, if provided\n",
        "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
        "                    progress_bar.update()\n",
        "                    if callback is not None and i % callback_steps == 0:\n",
        "                        callback(i, t, latents)\n",
        "\n",
        "        # If we do sequential model offloading, let's offload unet and controlnet\n",
        "        # manually for max memory savings\n",
        "        if hasattr(self, \"final_offload_hook\") and self.final_offload_hook is not None:\n",
        "            self.unet.to(\"cpu\")\n",
        "            self.controlnet.to(\"cpu\")\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        if output_type == \"latent\":\n",
        "            image = latents\n",
        "            has_nsfw_concept = None\n",
        "        elif output_type == \"pil\":\n",
        "            # 8. Post-processing\n",
        "            image = self.decode_latents(latents)\n",
        "\n",
        "            # 9. Run safety checker\n",
        "            image, has_nsfw_concept = self.run_safety_checker(image, device, prompt_embeds.dtype)\n",
        "\n",
        "            # 10. Convert to PIL\n",
        "            image = self.numpy_to_pil(image)\n",
        "        else:\n",
        "            # 8. Post-processing\n",
        "            image = self.decode_latents(latents)\n",
        "\n",
        "            # 9. Run safety checker\n",
        "            image, has_nsfw_concept = self.run_safety_checker(image, device, prompt_embeds.dtype)\n",
        "\n",
        "        # Offload last model to CPU\n",
        "        if hasattr(self, \"final_offload_hook\") and self.final_offload_hook is not None:\n",
        "            self.final_offload_hook.offload()\n",
        "\n",
        "        if not return_dict:\n",
        "            return (image, has_nsfw_concept)\n",
        "\n",
        "        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)"
      ],
      "metadata": {
        "id": "og7SvNTA-9ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzjUmMOwNXzq"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from diffusers import StableDiffusionImg2ImgPipeline, StableDiffusionDepth2ImgPipeline, StableDiffusionControlNetPipeline, ControlNetModel\n",
        "from diffusers import UniPCMultistepScheduler\n",
        "from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n",
        "from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import create_motion_field_and_warp_latents\n",
        "\n",
        "from controlnet_aux import LineartDetector, PidiNetDetector, HEDdetector, NormalBaeDetector\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "canny_controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_canny\", torch_dtype = torch.float16).to(device)\n",
        "normal_controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_normalbae\", torch_dtype = torch.float16).to(device)\n",
        "scribble_controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_scribble\", torch_dtype = torch.float16).to(device)\n",
        "lineart_controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_lineart\", torch_dtype = torch.float16).to(device)\n",
        "depth_controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11f1p_sd15_depth\", torch_dtype = torch.float16).to(device)\n",
        "\n",
        "pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", controlnet=canny_controlnet, torch_dtype=torch.float16\n",
        ").to(device)\n",
        "\n",
        "scribbleExtractor = HEDdetector.from_pretrained('lllyasviel/Annotators')\n",
        "lineartExtractor = LineartDetector.from_pretrained(\"lllyasviel/Annotators\")\n",
        "normalExtractor = NormalBaeDetector.from_pretrained(\"lllyasviel/Annotators\")\n",
        "depth_estimator = pipeline('depth-estimation')\n",
        "\n",
        "# import xformers\n",
        "# pipe.enable_xformers_memory_efficient_attention()\n",
        "# pipe.enable_vae_slicing()\n",
        "# pipe.unet.to(memory_format=torch.channels_last)\n",
        "\n",
        "pipe.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n",
        "pipe.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n",
        "pipe.safety_checker=None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "predictor = SamPredictor(sam)"
      ],
      "metadata": {
        "id": "Uf7Ds7f1SKA8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "-_TRxk9uPPA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9UQyvEagujZK"
      },
      "outputs": [],
      "source": [
        "def preprocess(image):\n",
        "    \n",
        "    w, h = image.size\n",
        "    print(w,h)\n",
        "    if w < h:\n",
        "      h = int(h*(512/w))\n",
        "      w = 512\n",
        "    else:\n",
        "      w = int(w*(512/h))\n",
        "      h = 512\n",
        "    w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 64, 32 can sometimes result in tensor mismatch errors\n",
        "    image = image.resize((w, h), resample=Image.LANCZOS)\n",
        "\n",
        "    canvas = Image.new(\"RGBA\", image.size, \"WHITE\")\n",
        "    canvas.paste(image, (0,0), image)\n",
        "    return_image = canvas\n",
        "    return_image = return_image.convert('RGB')\n",
        "\n",
        "    \n",
        "    \n",
        "    return return_image, image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_background_points(ori_image, gen_image):\n",
        "  ori_np = np.array(ori_image)\n",
        "  gen_np = np.array(gen_image)\n",
        "\n",
        "  ori_mask = ori_np[:,:,3]==0\n",
        "  gen_mask = np.all(gen_np>(230,230,230), axis=2)\n",
        "  desired_pixels = np.argwhere(ori_mask & gen_mask)\n",
        "\n",
        "  return desired_pixels\n",
        "\n",
        "def get_foreground_points(ori_image, gen_image):\n",
        "  ori_np = np.array(ori_image)\n",
        "  gen_np = np.array(gen_image)\n",
        "\n",
        "  ori_mask = ori_np[:,:,3]!=0\n",
        "  gen_mask = np.all(gen_np!=[255,255,255], axis=2)\n",
        "  desired_pixels = np.argwhere(ori_mask & gen_mask)\n",
        "\n",
        "  return desired_pixels\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def get_randomly_sampled_points(points, n):\n",
        "  sampled_rows = np.random.choice(points.shape[0], size=n, replace=False)\n",
        "  return points[sampled_rows, :]\n",
        "\n",
        "def get_maximal_diversity_points(points, n):\n",
        "    kmeans = KMeans(n_clusters=n, init='k-means++').fit(points)\n",
        "    distances = cdist(points, kmeans.cluster_centers_, 'euclidean')\n",
        "    max_distances = np.min(distances, axis=1)\n",
        "    indices = np.argsort(max_distances)[::-1][:n]\n",
        "    return points[indices]"
      ],
      "metadata": {
        "id": "neJ-_8_7W5zE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "def calc_mask(ori_image, images, N=50):\n",
        "  \n",
        "  t = datetime.now()\n",
        "  foreground_points = get_randomly_sampled_points(get_foreground_points(ori_image, images[0]), N)\n",
        "  background_points = get_background_points(ori_image, images[0])\n",
        "  background_points = get_randomly_sampled_points(background_points, N)\n",
        "\n",
        "\n",
        "  point_labels = [1]*N+[0]*N\n",
        "  point_coords = np.concatenate([foreground_points, background_points])\n",
        "  point_coords[:, [1,0]] = point_coords[:, [0,1]]\n",
        "  cv2_image = cv2.cvtColor(np.array(images[0]), cv2.COLOR_RGB2BGR)\n",
        "  predictor.set_image(cv2_image)\n",
        "  masks, scores, _ = predictor.predict(\n",
        "      point_coords=point_coords,\n",
        "      point_labels=point_labels,\n",
        "  )\n",
        "  idx = np.argmax(scores)\n",
        "  mask = masks[idx]\n",
        "\n",
        "  image = Image.fromarray(cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB))\n",
        "  alpha_channel = np.where(mask == False, 0, 255).astype('uint8')\n",
        "  image.putalpha(Image.fromarray(alpha_channel))\n",
        "  \n",
        "  return image\n",
        "\n"
      ],
      "metadata": {
        "id": "SOdan_TgXavi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "    \n",
        "def show_points(coords, labels, ax, marker_size=375):\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
        "    \n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))    \n"
      ],
      "metadata": {
        "id": "7EhDSlrCb6yy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_prompts(prompts, prompt_weights):\n",
        "  if len(prompts)==1 and len(prompt_weights)==0:\n",
        "    return prompts[0]\n",
        "  elif len(prompts)>1 or len(prompt_weights)==1:\n",
        "    text_input = pipe.tokenizer(prompts, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "      text_embeddings = pipe.text_encoder(text_input.input_ids.to(device))[0]\n",
        "\n",
        "    if len(prompt_weights) <=1:\n",
        "      text_embeddings = torch.mean(text_embeddings, dim=0)\n",
        "    elif len(prompt_weights) > 1:\n",
        "      prompt_weights = torch.Tensor(prompt_weights).to(device)\n",
        "      text_embeddings = torch.sum(text_embeddings * prompt_weights[:, None, None], dim=0)/torch.sum(prompt_weights)\n",
        "\n",
        "    return torch.reshape(text_embeddings, (1, text_embeddings.shape[0], text_embeddings.shape[1]))\n"
      ],
      "metadata": {
        "id": "ko0uKie5IfTt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cannyExtractor(init_image, low_threshold=100, high_threshold=200):\n",
        "  init_image = cv2.Canny(np.asarray(init_image), low_threshold, high_threshold)\n",
        "  init_image = init_image[:, :, None]\n",
        "  init_image = np.concatenate([init_image, init_image, init_image], axis=2)\n",
        "  init_image = Image.fromarray(init_image)\n",
        "  # init_image.show()\n",
        "  return init_image\n",
        "\n",
        "def depthExtractor(init_image, low_threshold=100, high_threshold=200):\n",
        "  init_image = depth_estimator(init_image)['depth']\n",
        "  init_image = np.array(init_image)\n",
        "  init_image = init_image[:, :, None]\n",
        "  init_image = np.concatenate([init_image, init_image, init_image], axis=2)\n",
        "  init_image = Image.fromarray(init_image)\n",
        "  return init_image"
      ],
      "metadata": {
        "id": "0QTx8dndSQJU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Dcnd443rmoC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import threading\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from flask import Flask, request, jsonify, send_file\n",
        "import json\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Response, Request\n",
        "from flask_cors import CORS, cross_origin\n",
        "\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.config['CORS_HEADERS'] = 'Content-Type'\n",
        "run_with_ngrok(app)\n",
        "# CORS(app)\n",
        "CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n",
        "from flask import Response\n",
        "\n",
        "@app.before_request\n",
        "def basic_authentication():\n",
        "  print(request.method)\n",
        "  if request.method.lower() == 'options':\n",
        "    return Response()\n",
        "\n",
        "def serve_pil_image(pil_img):\n",
        "    img_io = BytesIO()\n",
        "    pil_img.save(img_io, 'PNG', quality=70)\n",
        "    img_io.seek(0)\n",
        "    return send_file(img_io, mimetype='image/png')\n",
        "\n",
        "# Define Flask routes\n",
        "@app.route(\"/img2img\", methods=['POST'])\n",
        "@cross_origin()\n",
        "def img2img():\n",
        "    if request.method!='POST':\n",
        "      return\n",
        "    # print(request.method)\n",
        "    d = json.loads(request.get_data())\n",
        "    pipe_name = d['pipe']\n",
        "    prompt = d['prompt']\n",
        "    negative_prompt = d['negative_prompt']\n",
        "\n",
        "    prompt_weights = d['prompt_weights']\n",
        "    negative_prompt_weights = d['negative_prompt_weights']\n",
        "\n",
        "    strength=None\n",
        "    if 'strength' in d:\n",
        "      strength = d['strength']\n",
        "    cfg = d['cfg']\n",
        "\n",
        "    do_segment = d['segment']\n",
        "\n",
        "    seed = None\n",
        "    generator = None\n",
        "    if 'seed' in d:\n",
        "      seed = d['seed']\n",
        "      generator = generator = torch.Generator(device='cuda', )\n",
        "      generator.manual_seed(seed) \n",
        "    steps = d['steps']\n",
        "\n",
        "    prompt = [handle_prompts(prompt, prompt_weights)]\n",
        "    negative_prompt = [handle_prompts(negative_prompt, negative_prompt_weights)]\n",
        "\n",
        "    \n",
        "    ori_image = Image.open(BytesIO(base64.b64decode(d['init_img'].split(\",\",1)[1])))\n",
        "    init_image, ori_image = preprocess(ori_image)\n",
        "\n",
        "    first_frame_image = None\n",
        "    if 'first_frame' in d:\n",
        "      first_frame_image = Image.open(BytesIO(base64.b64decode(d['first_frame'].split(\",\",1)[1])))\n",
        "      first_frame_image, _ = preprocess(first_frame_image)\n",
        "      init_images = [first_frame_image, init_image]\n",
        "    else:\n",
        "      init_images = [init_image]\n",
        "\n",
        "    \n",
        "\n",
        "    control_images = None\n",
        "    if 'canny' in pipe_name:\n",
        "      control_images= [cannyExtractor(i) for i in init_images]\n",
        "      pipe.controlnet = canny_controlnet\n",
        "    elif 'lineart' in pipe_name:\n",
        "      control_images= [lineartExtractor(i) for i in init_images]\n",
        "      pipe.controlnet = lineart_controlnet\n",
        "    elif 'scribble' in pipe_name:\n",
        "      control_images= [scribbleExtractor(i) for i in init_images]\n",
        "      pipe.controlnet = scribble_controlnet\n",
        "    elif 'normal' in pipe_name:\n",
        "      control_images= [normalExtractor(i) for i in init_images]\n",
        "      pipe.controlnet = normal_controlnet\n",
        "    elif 'depth' in pipe_name:\n",
        "      control_images = [depthExtractor(i) for i in init_images]\n",
        "      pipe.controlnet = depth_controlnet\n",
        "\n",
        "    latents = torch.randn((1, 4, 64, 64), device=\"cuda\", dtype=torch.float16, generator= generator).repeat(len(init_images), 1, 1, 1)\n",
        "    if len(prompt_weights)==0 and len(negative_prompt_weights)==0:\n",
        "      images = pipe(prompt=prompt*len(init_images), negative_prompt=negative_prompt*len(init_images), latents=latents, init_image=init_image, controlnet_image=control_images, strength=strength, guidance_scale=cfg, num_inference_steps=steps, generator=generator).images\n",
        "    elif len(prompt_weights)!=0 and len(negative_prompt_weights)!=0:\n",
        "      images = pipe(prompt_embeds=prompt[0].repeat(len(init_images), 1, 1), negative_prompt_embeds=negative_prompt[0].repeat(len(init_images), 1, 1), latents=latents, init_image=init_image, controlnet_image=control_images, strength=strength, guidance_scale=cfg, num_inference_steps=steps, generator=generator).images\n",
        "\n",
        "\n",
        "    if do_segment:\n",
        "      return_image = calc_mask(ori_image, [images[len(images)-1]])\n",
        "    else:\n",
        "      return_image = images[len(images)-1]\n",
        "\n",
        "    buffered = BytesIO()\n",
        "    # images[0].show()\n",
        "    return_image.save(buffered, format=\"PNG\")\n",
        "    \n",
        "    output_img_send = base64.b64encode(buffered.getvalue())\n",
        "    output_img_send = output_img_send.decode(\"utf-8\")\n",
        "    \n",
        "    return jsonify({'img': 'data:image/png;base64,'+output_img_send})\n",
        "\n",
        "@app.route(\"/test\", methods=['GET'])\n",
        "def test():\n",
        "\n",
        "    return jsonify({'img': 'test'})\n",
        "# Start the Flask server in a new thread\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}